{
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
    //torchrun /usr/local/bin/torchrun
    "version": "0.2.0",
    "configurations": [
    
        {
            "name": "Python 调试程序: 包含参数的当前文件",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [],
            "cwd":"/root/Megatron-LM/",
            "env": {"PYTHONPATH":"/root/Megatron-LM",
                "PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION":"python"
            }
        },
        {
            "name": "torchrun调试程序: 包含参数的当前文件",
            "type": "debugpy",
            "request": "launch",
            "program": "/usr/local/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--nproc_per_node",
                "1",
                "--nnodes",
                "1",
                "--node_rank",
                "0",
                "--master_addr",
                "localhost",
                "--master_port",
                "6001",
                "${file}",
                "--tensor-model-parallel-size",
                "1",
                "--pipeline-model-parallel-size",
                "1",
                "--num-layers",
                "1",
                "--hidden-size",
                "768",
                "--num-attention-heads",
                "12",
                "--seq-length",
                "1024",
                "--max-position-embeddings",
                "1024",
                "--micro-batch-size",
                "12",
                "--global-batch-size",
                "192",
                "--lr",
                "0.0005",
                "--train-iters",
                "150000",
                "--lr-decay-iters",
                "150000",
                "--lr-decay-style",
                "cosine",
                "--lr-warmup-iters",
                "2000",
                "--weight-decay",
                ".1",
                "--adam-beta2",
                ".999",
                "--fp16",
                "--log-interval",
                "10",
                "--save-interval",
                "2000",
                "--eval-interval",
                "200",
                "--eval-iters",
                "10",
                "--vocab-file",
                "/root/Megatron-LM/experiments/gpt2_tokenizer_element/gpt2-vocab.json",
                "--merge-file",
                "/root/Megatron-LM/experiments/gpt2_tokenizer_element/gpt2-merges.txt",
                "--save",
                "/root/Megatron-LM/experiments/mygpt2",
                "--load",
                "/root/Megatron-LM/experiments/mygpt2",
                "--data-path",
                "/data/megatron-origin-dataset/my-gpt2_text_document",
                "--distributed-backend",
                "nccl",
                "--tensorboard-dir",
                "experiments/tensorboard"
            ],
            "env": {"CUDA_DEVICE_MAX_CONNECTIONS":"1",
            "PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION":"python",
            "CUDA_HOME":"/usr/local/cuda",
            "CUDA_VISIBLE_DEVICES":"3,2"
        
        }
        },
        {
            "name": "test调试程序: 当前文件",
            "type": "debugpy",
            "request": "launch",
            "program": "/usr/local/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--nproc_per_node",
                "2",
                "--nnodes",
                "1",
                "--node_rank",
                "0",
                "--master_addr",
                "localhost",
                "--master_port",
                "6001",
                "${file}"
            ]

        },
        {
            "name": "torchrun调试程序: gpt2评估",
            "type": "debugpy",
            "request": "launch",
            "program": "/usr/local/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--nproc_per_node",
                "1",
                "--nnodes",
                "1",
                "--node_rank",
                "0",
                "--master_addr",
                "localhost",
                "--master_port",
                "6015",
                "${file}",
                "--task",
                "LAMBADA",
                "--num-layers",
                "1",
                "--hidden-size",
                "768",
                "--num-attention-heads",
                "12",
                "--seq-length",
                "1024",
                "--max-position-embeddings",
                "1024",
                "--fp16",
                "--vocab-file",
                "/root/Megatron-LM/experiments/gpt2_tokenizer_element/gpt2-vocab.json",
                "--valid-data",
                "/data/lambada_origin/lambada_test.json",
                "--tokenizer-type",
                "GPT2BPETokenizer",
                "--merge-file",
                "/root/Megatron-LM/experiments/gpt2_tokenizer_element/gpt2-merges.txt",
                "--load",
                "/root/Megatron-LM/experiments/mygpt2",
                "--micro-batch-size",
                "1",
                "--log-interval",
                "10",
                "--no-load-optim",
                "--no-load-rng"
            ],
            "env": {"CUDA_DEVICE_MAX_CONNECTIONS":"1",
            "PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION":"python",
            "CUDA_HOME":"/usr/local/cuda",
            "CUDA_VISIBLE_DEVICES":"3,2"
        
        }
        }
    ]
}